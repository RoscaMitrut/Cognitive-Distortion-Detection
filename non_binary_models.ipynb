{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_data(model,labels,validation=False):\n",
    "\tfig_size_w = len(labels)\n",
    "\tif validation == True:\n",
    "\t\tval_to_plot = [model.history[\"val_\"+el] for el in labels]\n",
    "\tto_plot = [model.history[el] for el in labels]\n",
    "\n",
    "\tepochs = range(1, len(to_plot[0]) + 1)\n",
    "\n",
    "\tfig, axes = plt.subplots(1, fig_size_w,figsize=(20, 5))\n",
    "\tfig.tight_layout() \n",
    "\tfor i in range(0, fig_size_w):\n",
    "\t\taxes[i].plot(epochs, to_plot[i], '-', label=labels[i])\n",
    "\t\tif(validation == True):\n",
    "\t\t\taxes[i].plot(epochs, val_to_plot[i], ':', label=\"Validation \"+labels[i])\n",
    "\t\taxes[i].set_title(labels[i],fontsize=20)\n",
    "\t\taxes[i].legend(loc='lower right')\n",
    "\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distorsion_non_binary(row):  \n",
    "    if row[\"Dominant Distortion\"] == \"All-or-nothing thinking\":\n",
    "        return 1\n",
    "    if row[\"Dominant Distortion\"] == \"Overgeneralization\":\n",
    "        return 2\n",
    "    if row[\"Dominant Distortion\"] == \"Mental filter\":\n",
    "        return 3\n",
    "    if row[\"Dominant Distortion\"] == \"Should statements\":\n",
    "        return 4\n",
    "    if row[\"Dominant Distortion\"] == \"Labeling\":\n",
    "        return 5\n",
    "    if row[\"Dominant Distortion\"] == \"Personalization\":\n",
    "        return 6\n",
    "    if row[\"Dominant Distortion\"] == \"Magnification\":\n",
    "        return 7\n",
    "    if row[\"Dominant Distortion\"] == \"Emotional Reasoning\":\n",
    "        return 8\n",
    "    if row[\"Dominant Distortion\"] == \"Mind Reading\":\n",
    "        return 9\n",
    "    if row[\"Dominant Distortion\"] == \"Fortune-telling\":\n",
    "        return 10\n",
    "    return 0\n",
    "\n",
    "therapis_responses = pd.read_csv(\"data/Therapist_responses.csv\",delimiter=\",\")\n",
    "annotated_data = pd.read_csv(\"data/Annotated_data.csv\",delimiter=\",\")\n",
    "data = pd.merge(therapis_responses,annotated_data,on='Id_Number').drop([\"Question\"], axis=1)\n",
    "data[\"Distortion\"] = data.apply(label_distorsion_non_binary, axis=1)\n",
    "# 0 = ANSWER   ,   1 = ID   ,   2 = QUESTION   ,   3 = DISTORTED PART   ,\n",
    "# 4 = DOMINANT DISTORTION   ,   5 = SECONDARY DISTORTION   ,   6 = DISTORTION 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Distortion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_drop = 0\n",
    "column_to_check = 'Distortion'\n",
    "number_to_drop = 650  # Number of rows to drop with the specified value\n",
    "\n",
    "rows_with_value = data[data[column_to_check] == value_to_drop]\n",
    "\n",
    "# Step 4: Randomly select a subset of these rows to drop\n",
    "rows_to_drop = rows_with_value.sample(n=number_to_drop, random_state=42).index\n",
    "\n",
    "# Step 5: Drop the selected subset from the DataFrame\n",
    "data = data.drop(rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Distortion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    text = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if word.isalpha() and not word in stop_words]\n",
    "    return ' '.join(text)\n",
    "def find_max_list(list):\n",
    "    list_len = [len(i) for i in list]\n",
    "    return max(list_len)\n",
    "\n",
    "texts = data.values.tolist()\n",
    "\n",
    "sentences = [el[2] for el in texts]\n",
    "sentences = list(map(remove_stop_words,sentences))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LEN = len(tokenizer.word_index) + 1\n",
    "MAX_LEN = find_max_list(sequences)\n",
    "#MAX_LEN =200\n",
    "\n",
    "padded_sequences = pad_sequences(sequences,maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "#tokenizer.sequences_to_texts(padded_sequences)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = [el[-1] for el in texts]\n",
    "distortions = np.array(distortions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences, distortions)\n",
    "METRICS = [\n",
    "\ttf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "n_dim = 2\n",
    "model.add(Embedding(VOCAB_LEN, n_dim, input_length=MAX_LEN))#Vocabulary size of Tokenizer / Number of dimensions in embedding space / Length of padded sequence\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=METRICS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = tf.keras.utils.to_categorical(Y_train)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train,y_train_onehot,validation_data=(X_test,y_test_onehot),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_data(hist,[\"loss\",\"accuracy\",\"precision\",\"recall\"],validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_test)\n",
    "predicted = [np.argmax(el) for el in y_predicted]\n",
    "cm = confusion_matrix(Y_test, predicted)\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
