{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Flatten, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_data(model,labels,validation=False):\n",
    "\tfig_size_w = len(labels)\n",
    " \n",
    "\tif validation == True:\n",
    "\t\tval_to_plot = [model.history[\"val_\"+el] for el in labels]\n",
    "\tto_plot = [model.history[el] for el in labels]\n",
    "\n",
    "\tepochs = range(1, len(to_plot[0]) + 1)\n",
    "\n",
    "\tfig, axes = plt.subplots(1, fig_size_w,figsize=(20, 5))\n",
    "\tfig.tight_layout() \n",
    "\tfor i in range(0, fig_size_w):\n",
    "\t\taxes[i].plot(epochs, to_plot[i], '-', label=labels[i])\n",
    "\t\tif(validation == True):\n",
    "\t\t\taxes[i].plot(epochs, val_to_plot[i], ':', label=\"Validation \"+labels[i])\n",
    "\t\taxes[i].set_title(labels[i],fontsize=20)\n",
    "\t\taxes[i].legend(loc='lower right')\n",
    "\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_link = \"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\"\n",
    "encoder_link = \"https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-l-12-h-768-a-12/4\"\n",
    "\n",
    "preprocessor = hub.KerasLayer(preprocess_link)\n",
    "encoder = hub.KerasLayer(encoder_link)\n",
    "\n",
    "def get_embedings_sentences(sentences):\n",
    "\tpreprocessed_text = preprocessor(sentences)\n",
    "\treturn encoder(preprocessed_text)[\"pooled_output\"]\n",
    "\t#return encoder(preprocessed_text)[\"sequence_output\"]\n",
    " \n",
    "get_embedings_sentences([\"Hello fellow humans!\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    text = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if word.isalpha() and not word in stop_words]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def find_max_list(list):\n",
    "    list_len = [len(i) for i in list]\n",
    "    return max(list_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distorsion_binary(row):\n",
    "    if row[\"Dominant Distortion\"] == \"No Distortion\":\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "therapis_responses = pd.read_csv(\"data/Therapist_responses.csv\",delimiter=\",\")\n",
    "annotated_data = pd.read_csv(\"data/Annotated_data.csv\",delimiter=\",\")\n",
    "data = pd.merge(therapis_responses,annotated_data,on='Id_Number').drop([\"Question\"], axis=1)\n",
    "data[\"Distortion\"] = data.apply(label_distorsion_binary, axis=1)\n",
    "data_list = data.values.tolist()\n",
    "# 0 = ANSWER   ,   1 = ID   ,   2 = QUESTION   ,   3 = DISTORTED PART   ,\n",
    "# 4 = DOMINANT DISTORTION   ,   5 = SECONDARY DISTORTION   ,   6 = DISTORTION 1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data[\"Patient Question\"], data[\"Distortion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sw, X_test_sw, Y_train_sw, Y_test_sw = train_test_split(list(map(remove_stop_words,data[\"Patient Question\"])), data[\"Distortion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "\ttf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#BERT\n",
    "text_input = tf.keras.layers.Input(shape=(),dtype=tf.string,name=\"text\")\n",
    "preprocessed_text = preprocessor(text_input)\n",
    "outputs = encoder(preprocessed_text)\n",
    "#NN\n",
    "#layer = tf.keras.layers.Dense(10,activation=\"relu\")(outputs[\"pooled_output\"])\n",
    "#layer = tf.keras.layers.Dropout(0.25,name=\"dropout\")(layer)\n",
    "#layer = tf.keras.layers.Dense(1,activation=\"sigmoid\",name=\"output\")(layer)\n",
    "layer = tf.keras.layers.Dropout(0.05,name=\"dropout\")(outputs[\"pooled_output\"])\n",
    "layer = tf.keras.layers.Dense(1,activation=\"sigmoid\",name=\"output\")(layer)\n",
    "\n",
    "#Model\n",
    "model = tf.keras.Model(inputs=[text_input],outputs=[layer])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=METRICS)\n",
    "\n",
    "history = model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_data(history,[\"loss\",\"accuracy\",\"precision\",\"recall\"],True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data[\"Patient Question\"].tolist()\n",
    "sentences = list(map(remove_stop_words,sentences))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "VOCAB_LEN = len(tokenizer.word_index) + 1\n",
    "#MAX_LEN = find_max_list(sequences)\n",
    "MAX_LEN = 2000\n",
    "\n",
    "padded_sequences = pad_sequences(sequences,maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "#tokenizer.sequences_to_texts(padded_sequences)[:3]\n",
    "\n",
    "X_train_sw_tokenizer, X_test_sw_tokenizer, Y_train_sw_tokenizer, Y_test_sw_tokenizer = train_test_split(padded_sequences, np.array(data[\"Distortion\"]))\n",
    "\n",
    "# ANN\n",
    "model = Sequential() \n",
    "n_dim = 2\n",
    "model.add(Embedding(VOCAB_LEN, n_dim, input_length=MAX_LEN))#Vocabulary size of Tokenizer / Number of dimensions in embedding space / Length of padded sequence\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)\n",
    "model.summary()\n",
    "hist = model.fit(X_train_sw_tokenizer,Y_train_sw_tokenizer,validation_data=(X_test_sw_tokenizer,Y_test_sw_tokenizer),epochs=2)\n",
    "\n",
    "\n",
    "# CNN\n",
    "n_dim = 2\n",
    "seq_len = 3\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(VOCAB_LEN, n_dim, input_length=MAX_LEN))\n",
    "model2.add(Conv1D(n_dim, seq_len, activation='relu'))\n",
    "model2.add(MaxPooling1D(5))\n",
    "model2.add(Conv1D(n_dim, seq_len, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)\n",
    "model2.summary()\n",
    "hist2 = model2.fit(X_train_sw_tokenizer,Y_train_sw_tokenizer,validation_data=(X_test_sw_tokenizer,Y_test_sw_tokenizer),epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_data(hist,[\"loss\",\"accuracy\",\"precision\",\"recall\"],True)\n",
    "plot_train_val_data(hist2,[\"loss\",\"accuracy\",\"precision\",\"recall\"],True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data[\"Patient Question\"].tolist()\n",
    "distortions = data[\"Distortion\"].tolist()\n",
    "\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "#sbert_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "questions_embeded = sbert_model.encode(questions)\n",
    "\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(questions_embeded, distortions, test_size=0.2)\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=7, p=13, metric='euclidean')\n",
    "classifier.fit(X_train_bert,y_train_bert)\n",
    "\n",
    "classifier.score(X_test_bert,y_test_bert)\n",
    "\n",
    "y_predicted = classifier.predict(X_test_bert)\n",
    "predicted = [np.argmax(el) for el in y_predicted]\n",
    "cm = confusion_matrix(y_test_bert, predicted)\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_stopword = list(map(remove_stop_words,questions))\n",
    "\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "#sbert_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "questions_stopword_embeded = sbert_model.encode(questions)\n",
    "\n",
    "X_train_sw_bert, X_test_sw_bert, y_train_sw_bert, y_test_sw_bert = train_test_split(questions_stopword_embeded, distortions, test_size=0.2)\n",
    "\n",
    "classifier2 = KNeighborsClassifier(n_neighbors=7, p=13, metric='euclidean')\n",
    "classifier2.fit(X_train_sw_bert,y_train_sw_bert)\n",
    "\n",
    "classifier2.score(X_test_sw_bert,y_test_sw_bert)\n",
    "\n",
    "y_predicted = classifier2.predict(X_test_sw_bert)\n",
    "predicted = [np.argmax(el) for el in y_predicted]\n",
    "cm = confusion_matrix(y_test_sw_bert, predicted)\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(), metrics=METRICS)\n",
    "\n",
    "input_shape = X_train_bert.shape\n",
    "model.build(input_shape)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit(X_train_bert,np.array(y_train_bert),validation_data=(X_test_bert,np.array(y_test_bert)),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_data(hist,[\"loss\",\"accuracy\",\"precision\",\"recall\"],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_test_bert)\n",
    "predicted = [np.argmax(el) for el in y_predicted]\n",
    "cm = confusion_matrix(y_test_bert, predicted)\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
